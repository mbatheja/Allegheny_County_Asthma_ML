import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve, roc_auc_score
from sklearn.metrics import precision_score, recall_score, f1_score, average_precision_score, roc_auc_score

import warnings
warnings.filterwarnings('ignore')

from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline
from imblearn.combine import SMOTEENN, SMOTETomek


data = pd.read_csv('test.csv', index_col=0)
data = data.dropna()
data


data['AsthmaDiagnosisAge0to17Per100'].describe()


data['HighDiagnosisRate'] = (data['AsthmaDiagnosisAge0to17Per100']>=10).astype(int)
data[data['HighDiagnosisRate']==1]


selected_features = data.columns.to_list()[-10:-8]+data.columns.to_list()[-7:-1]
selected_features +=['NumberAcuteRespiratoryTractInfectionRelatedED_VisitsAge0to17','Total Population', 'American Indian and Alaska Native Population',
       'Some Other Race Population', 'Households', 'BPHIGH_CrudePrev',
       'COPD_CrudePrev', 'DIABETES_CrudePrev',
       '1,2-Ethanediol (Ethylene Glycol)', 'Acetaldehyde', 'Cadmium',
       'Chlorine', 'Chlorobenzene', 'Chloroform', 'Cyanides', 'Ethylbenzene',
       'Ethylene Oxide', 'Hexane', 'Hydrochloric Acid', 'Methane',
       'Nitrous Oxide (N2O)', 'Polycyclic Organic Matter', 'Sulfuric Acid',
       'Tetrachloroethylene (Perchloroethylene)', 'Toluene', 'Pre1950']
selected_features


X = data[selected_features]
y = data['HighDiagnosisRate']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

X_train.shape, X_test.shape


y.mean()*100  # percent


preprocessor = ColumnTransformer(
    transformers=[
        ('num', Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ]), selected_features)])


model_pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])


param_grid = {
    'classifier__n_estimators': [50, 100, 200, 500],
    'classifier__max_depth': [5, 10, 15, None],
    'classifier__min_samples_split': [2, 5, 10],
    'classifier__min_samples_leaf': [1, 2, 4],
    'classifier__bootstrap': [True, False]
}


demo_param_grid = {
    'classifier__n_estimators': [50, 100],
    'classifier__max_depth': [5, 10],
    'classifier__min_samples_split': [2, 5],
    'classifier__min_samples_leaf': [1, 2],
    'classifier__bootstrap': [True]
}


cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


grid_search = GridSearchCV (
    estimator  = model_pipeline,
    param_grid = demo_param_grid,  # Use demo_param_grid for quicker execution
            cv = cv,
    #scoring='precision_recall_curve',
         refit = 'PR_AUC',  # Refit based on PR AUC
        n_jobs = -1,
       verbose = 2,
    return_train_score=True,
    scoring={
        'accuracy': 'accuracy',
        'precision': 'precision',
        'recall': 'recall',
        'f1': 'f1',
        'roc_auc': 'roc_auc',
        'PR_AUC': 'average_precision'  # PR AUC score
    }
)


print("\nStarting grid search...")
# Fit the grid search to the data
grid_search.fit(X_train, y_train)

# Print best parameters and score
print("\nBest parameters:")
print(grid_search.best_params_)
print(f"\nBest cross-validation score: {grid_search.best_score_:.4f}")

# Get the best model
best_model = grid_search.best_estimator_

# Evaluate on test set
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)[:, 1]

print("\nTest Set Evaluation:")
print(classification_report(y_test, y_pred))


plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.png')


roc_auc = roc_auc_score(y_test, y_prob)
roc_auc


precision, recall, _ = precision_recall_curve(y_test, y_prob)
pr_auc = auc(recall, precision)
pr_auc


plt.figure(figsize=(10, 8))
plt.subplot(2, 1, 1)
fpr, tpr, _ = roc_curve(y_test, y_prob)
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve for Diagnosis Prediction')
plt.legend(loc='lower right')


#plt.subplot(2, 1, 2)
#plt.plot(recall, precision, label=f'PR Curve (AUC = {pr_auc:.4f})')
#plt.xlabel('Recall')
#plt.ylabel('Precision')
#plt.title('Precision-Recall Curve for Diagnosis Prediction')
#plt.legend(loc='upper right')
#plt.tight_layout()





def evaluate_model( msg, X_test, y_test, y_pred, y_proba ):
    w=18
    print(msg, '\n')
    cm = confusion_matrix(y_test, y_pred)
    print( cm ); print()
    print( f'%{w}s : %.4f' % ('Precision', precision_score(y_test, y_pred)))
    print( f'%{w}s : %.4f' % ('Recall', recall_score(y_test, y_pred)))
    print( f'%{w}s : %.4f' % ('F1 Score', f1_score(y_test, y_pred)))
    print( f'%{w}s : %.4f' % ('AUC-ROC', roc_auc_score(y_test, y_prob)))
    print( f'%{w}s : %.4f' % ('Average Precision', average_precision_score( y_test, y_prob)))
    print()
    
evaluate_model('Baseline Model Performance', X_test, y_test, y_pred, y_prob)

# Now apply imbalanced-learn techniques

# 1. SMOTE (Oversampling)
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Check new class distribution
print(f"\nBefore SMOTE - Training examples: {len(X_train)}")
print(f"Class distribution: {np.bincount(y_train)}")

# Check new class distribution
print(f"\nAfter SMOTE - Training examples: {len(X_train_smote)}")
print(f"Class distribution: {np.bincount(y_train_smote)}")


clf_smote = RandomForestClassifier(random_state=42)
clf_smote.fit(X_train_smote, y_train_smote)
y_pred_smote = clf_smote.predict(X_test)
y_proba_smote = clf_smote.predict_proba(X_test)





def evaluate_model( msg, X_test, y_test, y_pred, y_proba ):
    w=18
    print(msg, '\n')
    cm = confusion_matrix(y_test, y_pred)
    print( cm ); print()
    print( f'%{w}s : %.4f' % ('Precision', precision_score(y_test, y_pred)))
    print( f'%{w}s : %.4f' % ('Recall', recall_score(y_test, y_pred)))
    print( f'%{w}s : %.4f' % ('F1 Score', f1_score(y_test, y_pred)))
    print( f'%{w}s : %.4f' % ('AUC-ROC', roc_auc_score(y_test, y_proba[:,1])))
    print( f'%{w}s : %.4f' % ('Average Precision', average_precision_score( y_test, y_proba[:,1] )))
    print()
    
evaluate_model('SMOTE Model Performance', X_test, y_test, y_pred_smote, y_proba_smote)



rus = RandomUnderSampler(random_state=42)
X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

# Check new class distribution
print(f"\nAfter Undersampling - Training examples: {len(X_train_rus)}")
print(f"Class distribution: {np.bincount(y_train_rus)}")


clf_rus = RandomForestClassifier(random_state=42)
clf_rus.fit(X_train_rus, y_train_rus)
y_pred_rus = clf_rus.predict(X_test)
y_proba_rus = clf_rus.predict_proba(X_test)


evaluate_model('Undersample Model Performance', X_test, y_test, y_pred_rus, y_proba_rus )


plt.figure(figsize=(10, 8))

# Baseline model PR curve
precision_base, recall_base, _ = precision_recall_curve(y_test, y_prob)
ap_base = average_precision_score(y_test, y_prob)
plt.plot(recall_base, precision_base, label=f'Baseline (AP = {ap_base:.3f})')

# SMOTE model PR curve
precision_smote, recall_smote, _ = precision_recall_curve(y_test, y_proba_smote[:, 1])
ap_smote = average_precision_score(y_test, y_proba_smote[:, 1])
plt.plot(recall_smote, precision_smote, label=f'SMOTE (AP = {ap_smote:.3f})')

# Undersampling model PR curve
precision_rus, recall_rus, _ = precision_recall_curve(y_test, y_proba_rus[:, 1])
ap_rus = average_precision_score(y_test, y_proba_rus[:, 1])
plt.plot(recall_rus, precision_rus, label=f'Undersampling (AP = {ap_rus:.3f})')

plt.title('Precision-Recall Curve')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.legend()
plt.grid(True)
plt.show()



