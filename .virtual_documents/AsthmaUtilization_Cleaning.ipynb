


# inspect dataset


!pip install geopandas
!pip install shapely


import pandas as pd
# pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)
import geopandas as gpd
from shapely.geometry import Point
from shapely.validation import make_valid
import numpy as np
import matplotlib.pyplot as plt





alCoCTs = pd.read_csv('our-data/AlleghenyCountyCensusTracts2016.csv')
print('Number of Census Tracts: ', len(alCoCTs))
alCoCTs.sort_values('geoid')


alCoCTs['geoid'].nunique()


alCoBGs = pd.read_csv('our-data/AlleghenyCountyCensusBlockGroups2016.csv')
print('Number of Block Groups: ', len(alCoBGs))
alCoBGs.sort_values('geoid')


alCoBGs['geoid'].nunique()





#Loading in 2020 data to fill in missing rows and filtering on Allegheny County values
placesData = pd.read_csv('our-data/PLACES__Census_Tract_Data__GIS_Friendly_Format___2020_release_20250417.csv')
placesData = placesData[placesData['CountyName']=='Allegheny']

#Loading in data for 2019
cities2019 = pd.read_csv("our-data/500_Cities__Census_Tract-level_Data__GIS_Friendly_Format___2019_release_20250405.csv")
#Filtering for Pittsburgh census tracts
cities2019=cities2019[cities2019['PlaceName'] == 'Pittsburgh']
#Removing values from the 2020 data set that are already in the 2019 dataset
places2019 = placesData[placesData['TractFIPS'].apply(lambda x: x not in cities2019['TractFIPS'].to_list())]
#Joining to represent all Census Tracts in Allegheny County and adding column 'Year' indicating time period that each row represents
prevn2019 = pd.concat([cities2019, places2019])
prevn2019['Year'] = ['2019']*len(prevn2019)

#Above process is repeated for the remaining years
cities2018 = pd.read_csv("our-data/500_Cities__Census_Tract-level_Data__GIS_Friendly_Format___2018_release_20250416.csv")
cities2018=cities2018[cities2018['PlaceName'] == 'Pittsburgh']
places2018= placesData[placesData['TractFIPS'].apply(lambda x: x not in cities2018['TractFIPS'].to_list())]
prevn2018 = pd.concat([cities2018, places2018])
prevn2018['Year'] = ['2018']*len(prevn2018)

cities2017 = pd.read_csv("our-data/500_Cities__Census_Tract-level_Data__GIS_Friendly_Format___2017_release_20250416.csv")
cities2017=cities2017[cities2017['PlaceName'] == 'Pittsburgh']
places2017= placesData[placesData['TractFIPS'].apply(lambda x: x not in cities2017['TractFIPS'].to_list())]
prevn2017 = pd.concat([cities2017, places2017])
prevn2017['Year'] = ['2017']*len(prevn2017)

cities2016 = pd.read_csv("our-data/500_Cities__Census_Tract-level_Data__GIS_Friendly_Format___2016_release_20250416.csv")
cities2016=cities2016[cities2016['PlaceName'] == 'Pittsburgh']
places2016= placesData[placesData['TractFIPS'].apply(lambda x: x not in cities2016['TractFIPS'].to_list())]
prevn2016 = pd.concat([cities2016, places2016])
prevn2016['Year'] = ['2016']*len(prevn2016)

#Data for each year is joined together after every year is completed
cond_prevn = pd.concat([prevn2019, prevn2018, prevn2017, prevn2016])
cond_prevn


print("Unique tract IDs: " + str(cond_prevn['TractFIPS'].nunique()))
print("Shape: " + str(cond_prevn.shape))





cond_prevn = cond_prevn.rename(columns={'TractFIPS': 'Geo_FIPS'})
#Removing unrelated prevalence columns (not comorbidities with Asthma)
def drop_columns_with_keyword(df, keyword):
    # Identify columns containing the keyword (95CI)
    cols_to_drop = [col for col in df.columns if keyword.lower() in col.lower()]
    # Drop them and return the updated DataFrame
    return df.drop(columns=cols_to_drop)
cond_prevn = drop_columns_with_keyword(cond_prevn,'95CI')
cond_prevn = cond_prevn.drop([
    'ARTHRITIS_CrudePrev',
    'BINGE_CrudePrev',
    'CANCER_CrudePrev',
    'COLON_SCREEN_CrudePrev',
    'DENTAL_CrudePrev',
    'KIDNEY_CrudePrev',
    'LPA_CrudePrev',
    'MAMMOUSE_CrudePrev',
    'MHLTH_CrudePrev',
    'PAPTEST_CrudePrev',
    'PHLTH_CrudePrev',
    'SLEEP_CrudePrev',
    'STROKE_CrudePrev',
    'TEETHLOST_CrudePrev'], axis=1)
cond_prevn['lat'] = cond_prevn['Geolocation'].str[1:15]
cond_prevn['lon'] = cond_prevn['Geolocation'].str[-15:-1]
cond_prevn


print(cond_prevn.shape)
cond_prevn.isna().sum()





#Renaming dataframe to be more descriptive
prevalence_data = cond_prevn

#Dropping columns containing NaNs
prevalence_data = prevalence_data.drop(columns=["COREW_CrudePrev", 'StateAbbr', 'PlaceName',
                                                'PlaceFIPS', 'Place_TractID', 'StateDesc', 'CountyName',
                                                'CountyFIPS', 'TotalPopulation', 'CERVICAL_CrudePrev',
                                                'Population2010', 'population_count'])

# Keeping only census tract level identifier for Geo_FIPS to facilitate later joins
prevalence_data['Geo_FIPS'] = prevalence_data['Geo_FIPS'].apply(lambda x: int(str(x)[-6:]))

#Dataframe is now free of missing values
prevalence_data.isna().sum()





ER_use = pd.read_csv("our-data/Allegheny_county_ER_use.csv")
ER_use


ER_use.shape


PC_use = pd.read_csv("our-data/Allegheny_county_PC_use.csv")
# PC_use = PC_use.fillna(0)
PC_use


PC_use.shape





ER_use['TractFIPS'].nunique() # 402 tracts in ER data
PC_use['TractFIPS'].nunique() # 397 tracts in PC data

# Get number of occurrences of each tract + blockgroup combination; we want to see each combination appear 4 times (for the four years 2016-19)
ER_combo_counts = ER_use.groupby(['TractFIPS', 'BlockgroupFIPS']).size().reset_index(name = 'Count').sort_values('Count', ascending = False)
ER_combo_counts.tail(20)

PC_combo_counts = PC_use.groupby(['TractFIPS', 'BlockgroupFIPS']).size().reset_index(name = 'Count').sort_values('Count', ascending = False)





# Bring in all Allegheny County blockgroups so we can ensure we have all of them
alCoBGs.head()
alCoBGs_subset = alCoBGs[['geoid', 'tract_ce', 'blkgrp_ce']]
alCoBGs_subset.head()
alCoBGs_subset['tract_ce'].nunique()


ER_use[ER_use['TractFIPS'] == 981100]
ER_use[ER_use['YearOfStartDate'].isna()]

PC_use[PC_use['YearOfContactDate'].isna()]


# Add placeholder rows for tracts + blockgroups that are missing
# First need to add years 2016-19 to base blockgroup data frame
years = pd.DataFrame({'year': [2016, 2017, 2018, 2019]})
alCoBGs_subset['key'] = 1 # Use key columns to join
years['key'] = 1
base_df = alCoBGs_subset.merge(years, on = 'key').drop(columns = 'key')

# Merge base data frame containing all tracts, blockgroups, and years with the ER use dataset
ER_expanded = base_df.merge(ER_use, how = 'left', left_on = ['tract_ce', 'blkgrp_ce', 'year'], right_on = ['TractFIPS', 'BlockgroupFIPS', 'YearOfStartDate']).sort_values(by = '_id')
ER_expanded.tail(50)
ER_expanded['Geo_FIPS'] = ER_expanded['Geo_FIPS'].fillna(ER_expanded['geoid'])
ER_expanded['TractFIPS'] = ER_expanded['TractFIPS'].fillna(ER_expanded['tract_ce'])
ER_expanded['BlockgroupFIPS'] = ER_expanded['BlockgroupFIPS'].fillna(ER_expanded['blkgrp_ce'])
ER_expanded['YearOfStartDate'] = ER_expanded['YearOfStartDate'].fillna(ER_expanded['year'])
ER_expanded['StateFIPS'] = ER_expanded['StateFIPS'].fillna(42)
ER_expanded['CountyFIPS'] = ER_expanded['CountyFIPS'].fillna(3)
ER_expanded.shape

# Merge base + ER data frame with the PC use dataset
merged_data = ER_expanded.merge(PC_use, how = 'left', left_on = ['tract_ce', 'blkgrp_ce', 'year'], right_on = ['TractFIPS', 'BlockgroupFIPS', 'YearOfContactDate'], suffixes=('_er', '_pc'))
merged_data.head(13)
util_data = merged_data.drop(columns = ['tract_ce', 'blkgrp_ce', 'year', '_id_er', '_id_pc', 'Geo_FIPS_pc', 'StateFIPS_pc', 'CountyFIPS_pc', 'TractFIPS_pc', 'BlockgroupFIPS_pc', 'TotalPopEst2015_19ACS_pc', 'Age0to17PopEst2015_19ACS_pc'])
util_data = util_data.rename(columns = {'Geo_FIPS_er':'Geo_FIPS', 'StateFIPS_er':'StateFIPS', 'CountyFIPS_er':'CountyFIPS', 'TractFIPS_er':'TractFIPS', 'BlockgroupFIPS_er':'BlockgroupFIPS', 'TotalPopEst2015_19ACS_er':'TotalPopEst2015_19ACS'})
util_data.shape


#merging utilization data for Asthma ER and primary care
util_data = PC_use.merge(ER_use, how = 'left', left_on = ['Geo_FIPS', 'YearOfContactDate'], right_on = ['Geo_FIPS', 'YearOfStartDate'], validate='one_to_one')
#dropping repeat columns
util_data = util_data.drop(["_id_y","StateFIPS_y","CountyFIPS_y","TractFIPS_y","BlockgroupFIPS_y",
                            "TotalPopEst2015_19ACS_y","Age0to17PopEst2015_19ACS_y"],axis="columns")
util_data


util_data.shape


#Joining rows based on CTs
#Dropping Per100 columns
util_data = drop_columns_with_keyword(util_data,'Per100')
util_data['Geo_FIPS'] = util_data['Geo_FIPS'].apply(str)
util_data['Geo_FIPS'] = util_data['Geo_FIPS'].str[:-1]
#Changing Geo_FIPS to contain only Census Tract identifier
util_data['Geo_FIPS'] = util_data['Geo_FIPS'].apply(lambda x: int(x[-6:]))
util_data = util_data.drop(columns=['_id_x', 'StateFIPS_x', 'CountyFIPS_x', 'TractFIPS_x', 'BlockgroupFIPS_x'])
util_data = util_data.groupby(['Geo_FIPS', 'YearOfContactDate'], as_index=False).sum()
util_data.isna().sum()


util_data





#Census data has to be loaded in for each year in 2016-19
#Census indicates missing values using -
demo2016 = pd.read_csv('our-data/ACSDP5Y2016.DP05-Data.csv', header = 1, na_values='-')
#Adding in timeperiod indicator
demo2016['Year'] = [2016]*len(demo2016)
#Filtering out data that does not pertain to Census Tract identifiers, Median Age,
#or estimates for number of individuals from each racial group
demo2016 = demo2016[['Geography', 'Year',
          'Estimate!!SEX AND AGE!!Median age (years)',
          'Estimate!!RACE!!Total population',
          'Estimate!!RACE!!One race!!White',
          'Estimate!!RACE!!One race!!Black or African American',
          'Estimate!!RACE!!One race!!Asian',
          'Estimate!!RACE!!One race!!American Indian and Alaska Native',
          'Estimate!!RACE!!One race!!Native Hawaiian and Other Pacific Islander',
          'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)',
          'Estimate!!RACE!!One race!!Some other race',
          'Estimate!!RACE!!Two or more races']]
#Renaming columns to be more easily legible
demo2016 = demo2016.rename(columns = {'Estimate!!SEX AND AGE!!Median age (years)':'Median Age',
                           'Estimate!!RACE!!Total population':'Total Population',
                           'Estimate!!RACE!!One race!!White':'White Population',
                           'Estimate!!RACE!!One race!!Black or African American':'Black or African American Population',
                           'Estimate!!RACE!!One race!!Asian':'Asian Population',
                           'Estimate!!RACE!!One race!!American Indian and Alaska Native': 'American Indian and Alaska Native Population',
                           'Estimate!!RACE!!One race!!Native Hawaiian and Other Pacific Islander':'Native Hawaiian and Other Pacific Islander Population',
                           'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)':'Hispanic or Latino Population',
                           'Estimate!!RACE!!One race!!Some other race':'Some Other Race Population',
                           'Estimate!!RACE!!Two or more races':'Two or More Races Population'})

#Process is repeated for 2017, 2018, and 2019 but column names in original dataset change in these years
demo2017 = pd.read_csv('our-data/ACSDP5Y2017.DP05-Data.csv', header = 1, na_values='-')
demo2017['Year'] = [2017]*len(demo2017)
#Filtering reflects different name for columns. The real world value that is being estimated stays constant despite name change.
demo2017 = demo2017[['Geography', 'Year',
          'Estimate!!SEX AND AGE!!Total population!!Median age (years)',
          'Estimate!!RACE!!Total population',
          'Estimate!!RACE!!Total population!!One race!!White',
          'Estimate!!RACE!!Total population!!One race!!Black or African American',
          'Estimate!!RACE!!Total population!!One race!!Asian',
          'Estimate!!RACE!!Total population!!One race!!American Indian and Alaska Native',
          'Estimate!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander',
          'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)',
          'Estimate!!RACE!!Total population!!One race!!Some other race',
          'Estimate!!RACE!!Total population!!Two or more races']]
demo2017 = demo2017.rename(columns = {'Estimate!!SEX AND AGE!!Total population!!Median age (years)':'Median Age',
                           'Estimate!!RACE!!Total population':'Total Population',
                           'Estimate!!RACE!!Total population!!One race!!White':'White Population',
                           'Estimate!!RACE!!Total population!!One race!!Black or African American':'Black or African American Population',
                           'Estimate!!RACE!!Total population!!One race!!Asian':'Asian Population',
                           'Estimate!!RACE!!Total population!!One race!!American Indian and Alaska Native': 'American Indian and Alaska Native Population',
                           'Estimate!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander':'Native Hawaiian and Other Pacific Islander Population',
                           'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)':'Hispanic or Latino Population',
                           'Estimate!!RACE!!Total population!!One race!!Some other race':'Some Other Race Population',
                           'Estimate!!RACE!!Total population!!Two or more races':'Two or More Races Population'})

demo2018 = pd.read_csv('our-data/ACSDP5Y2018.DP05-Data.csv', header = 1, na_values='-')
demo2018['Year'] = [2018]*len(demo2018)
demo2018 = demo2018[['Geography', 'Year',
          'Estimate!!SEX AND AGE!!Total population!!Median age (years)',
          'Estimate!!RACE!!Total population',
          'Estimate!!RACE!!Total population!!One race!!White',
          'Estimate!!RACE!!Total population!!One race!!Black or African American',
          'Estimate!!RACE!!Total population!!One race!!Asian',
          'Estimate!!RACE!!Total population!!One race!!American Indian and Alaska Native',
          'Estimate!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander',
          'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)',
          'Estimate!!RACE!!Total population!!One race!!Some other race',
          'Estimate!!RACE!!Total population!!Two or more races']]
demo2018 = demo2018.rename(columns = {'Estimate!!SEX AND AGE!!Total population!!Median age (years)':'Median Age',
                           'Estimate!!RACE!!Total population':'Total Population',
                           'Estimate!!RACE!!Total population!!One race!!White':'White Population',
                           'Estimate!!RACE!!Total population!!One race!!Black or African American':'Black or African American Population',
                           'Estimate!!RACE!!Total population!!One race!!Asian':'Asian Population',
                           'Estimate!!RACE!!Total population!!One race!!American Indian and Alaska Native': 'American Indian and Alaska Native Population',
                           'Estimate!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander':'Native Hawaiian and Other Pacific Islander Population',
                           'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)':'Hispanic or Latino Population',
                           'Estimate!!RACE!!Total population!!One race!!Some other race':'Some Other Race Population',
                           'Estimate!!RACE!!Total population!!Two or more races':'Two or More Races Population'})

demo2019 = pd.read_csv('our-data/ACSDP5Y2019.DP05-Data.csv', header = 1, na_values='-')
demo2019['Year'] = [2019]*len(demo2019)
demo2019 = demo2019[['Geography', 'Year',
          'Estimate!!SEX AND AGE!!Total population!!Median age (years)',
          'Estimate!!RACE!!Total population',
          'Estimate!!RACE!!Total population!!One race!!White',
          'Estimate!!RACE!!Total population!!One race!!Black or African American',
          'Estimate!!RACE!!Total population!!One race!!Asian',
          'Estimate!!RACE!!Total population!!One race!!American Indian and Alaska Native',
          'Estimate!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander',
          'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)',
          'Estimate!!RACE!!Total population!!One race!!Some other race',
          'Estimate!!RACE!!Total population!!Two or more races']]
demo2019 = demo2019.rename(columns = {'Estimate!!SEX AND AGE!!Total population!!Median age (years)':'Median Age',
                           'Estimate!!RACE!!Total population':'Total Population',
                           'Estimate!!RACE!!Total population!!One race!!White':'White Population',
                           'Estimate!!RACE!!Total population!!One race!!Black or African American':'Black or African American Population',
                           'Estimate!!RACE!!Total population!!One race!!Asian':'Asian Population',
                           'Estimate!!RACE!!Total population!!One race!!American Indian and Alaska Native': 'American Indian and Alaska Native Population',
                           'Estimate!!RACE!!Total population!!One race!!Native Hawaiian and Other Pacific Islander':'Native Hawaiian and Other Pacific Islander Population',
                           'Estimate!!HISPANIC OR LATINO AND RACE!!Total population!!Hispanic or Latino (of any race)':'Hispanic or Latino Population',
                           'Estimate!!RACE!!Total population!!One race!!Some other race':'Some Other Race Population',
                           'Estimate!!RACE!!Total population!!Two or more races':'Two or More Races Population'})

#Joining data for all four years to create one dataframe
demo_data = pd.concat([demo2016, demo2017, demo2018, demo2019], axis = 0)
demo_data


#There are values for all 402 census tracts 35 rows are missing values for Median Age
demo_data.isna().sum()


#Missing values are in census tract with either no population or very low population
demo_data[demo_data['Median Age'].isna()]





#Census data has to be loaded in for each year in 2016-19
#Census indicates missing values using -
income2016 = pd.read_csv('our-data/ACSST5Y2016.S1903-Data.csv', header = 1, na_values='-')
#Adding time period indicator for each row
income2016['Year'] = [2016]*len(income2016)
#Filtering out columns that do not contain row identifier, number of households, or median income
income2016 = income2016[['Geography', 'Year', 'Total!!Estimate!!Households',
                         'Median income (dollars)!!Estimate!!Households']]
#Renaming columns to be more legible
income2016 = income2016.rename(columns={'Total!!Estimate!!Households':'Households',
                                        'Median income (dollars)!!Estimate!!Households':'Median Income'})

#Process is repeated for 2017, 2018, and 2019 but column names in original dataset change in these years
income2017 = pd.read_csv('our-data/ACSST5Y2017.S1903-Data.csv', header = 1, na_values='-')
income2017['Year'] = [2017]*len(income2017)
#Although names of columns change, same values are still being estimated
income2017 = income2017[['Geography', 'Year',
  'Estimate!!Number!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households',
  'Estimate!!Median income (dollars)!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households']]
income2017 = income2017.rename(columns={'Estimate!!Number!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households':'Households',
                           'Estimate!!Median income (dollars)!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households':'Median Income'})

income2018 = pd.read_csv('our-data/ACSST5Y2018.S1903-Data.csv', header = 1, na_values='-')
income2018['Year'] = [2018]*len(income2018)
income2018 = income2018[['Geography', 'Year',
  'Estimate!!Number!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households',
  'Estimate!!Median income (dollars)!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households']]
income2018 = income2018.rename(columns={'Estimate!!Number!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households':'Households',
                           'Estimate!!Median income (dollars)!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households':'Median Income'})

income2019 = pd.read_csv('our-data/ACSST5Y2019.S1903-Data.csv', header = 1, na_values='-')
income2019['Year'] = [2019]*len(income2019)
income2019 = income2019[['Geography', 'Year',
  'Estimate!!Number!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households',
  'Estimate!!Median income (dollars)!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households']]
income2019 = income2019.rename(columns={'Estimate!!Number!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households':'Households',
                           'Estimate!!Median income (dollars)!!HOUSEHOLD INCOME BY RACE AND HISPANIC OR LATINO ORIGIN OF HOUSEHOLDER!!Households':'Median Income'})

#Joining data for all four years to create one dataframe
income_data=pd.concat([income2016,income2017,income2018,income2019])
income_data


#Missing values for 50 census tracts
income_data.isna().sum()


#All but one of these census tracts has either no households or a very small number or households
income_data[income_data['Median Income'].isna()]





census_data = demo_data.merge(income_data, how='inner', on=['Geography', 'Year'], validate='one_to_one')
#Creating indentifier that contains only census tract information
census_data['Geo_FIPS'] = census_data['Geography'].apply(lambda x: int(x[-6:]))
census_data


#Same values contain NaNs
census_data.isna().sum()








emissions_data = pd.read_csv('our-data/final_emissions_data.csv', index_col=0)
emissions_data





#Merging health care utlization data with census data on race and household income
census_data['Geo_FIPS'] = census_data['Geo_FIPS'].astype(int)
utilxcensus_data = util_data.merge(census_data, how='left', left_on=['Geo_FIPS','YearOfContactDate'], right_on=['Geo_FIPS','Year'])
utilxcensus_data['Geo_FIPS']=utilxcensus_data['Geo_FIPS'].apply(int)
utilxcensus_data


#merged census and utilization data with health condition prevalence data
utilxcensus_data = utilxcensus_data.dropna(subset=['YearOfContactDate'])
utilxcensus_data['YearOfContactDate'] = utilxcensus_data['YearOfContactDate'].astype(int)
prevalence_data['Year']= prevalence_data['Year'].astype(int)
semi_data = utilxcensus_data.merge(prevalence_data, how='right', left_on=['Geo_FIPS', 'Year'], right_on=['Geo_FIPS','Year'])
semi_data


#joining emissions dataset
data = semi_data.merge(emissions_data, how='inner', left_on=['Geo_FIPS', 'YearOfContactDate'], right_on=['TRACTCE','Year'])
data


#Filtering out columns that are not important and then downloading csv
cols = ['Geo_FIPS','YearOfContactDate',
 'TotalPopEst2015_19ACS_x',
 'Age0to17PopEst2015_19ACS_x',
 'AllPrimaryCarePatientsAge0to17',
 'WellChildVisitsInPastYearAge0to17',
 'AsthmaDiagnosisAge0to17',
 'UnderAge1PopEst2015_19ACS',
 'Unique0to17WithED_Visit',
 'NumberED_VisitsAge0to17',
 'NumberLowAcuityED_VisitsAge0to17',
 'NumberAsthmaRelatedED_Visits',
 'NumberED_VisitsByChildrenUnder1YearOld',
 'NumberOfInjuryRelatedVisits',
 'NumberAcuteRespiratoryTractInfectionRelatedED_VisitsAge0to17',
 'Median Age',
 'Total Population',
 'White Population',
 'Black or African American Population',
 'Asian Population',
 'American Indian and Alaska Native Population',
 'Native Hawaiian and Other Pacific Islander Population',
 'Hispanic or Latino Population',
 'Some Other Race Population',
 'Two or More Races Population',
 'Households',
 'Median Income',
 'ACCESS2_CrudePrev',
 'BPHIGH_CrudePrev',
 'BPMED_CrudePrev',
 'CASTHMA_CrudePrev',
 'CHD_CrudePrev',
 'CHECKUP_CrudePrev',
 'CHOLSCREEN_CrudePrev',
 'COPD_CrudePrev',
 'COREM_CrudePrev',
 'CSMOKING_CrudePrev',
 'DIABETES_CrudePrev',
 'HIGHCHOL_CrudePrev',
 'OBESITY_CrudePrev',
 '1,1,2-Trichloroethane',
 '1,1,2,2-Tetrachloroethane',
 '1,2-Ethanediol (Ethylene Glycol)',
 '1,3-Butadiene',
 '1,4-Dichlorobenzene',
 '1,4-Dioxane (1,4-Diethyleneoxide)',
 '2,2,4-Trimethylpentane',
 'Acetaldehyde',
 'Acrolein',
 'Acrylic Acid',
 'Acrylonitrile',
 'Ammonia',
 'Antimony',
 'Arsenic',
 'Benzene',
 'Cadmium',
 'Carbon Dioxide',
 'Carbon Disulfide',
 'Carbon Monoxide',
 'Carbon Tetrachloride',
 'Chlorine',
 'Chlorobenzene',
 'Chloroethene (vinyl chloride)',
 'Chloroform',
 'Chromium',
 'Cobalt',
 'Cresols/Cresylic Acid (Isomers And Mixture)',
 'Cyanides',
 'Dibutylphthalate',
 'Ethyl Chloride (Chloroethane)',
 'Ethylbenzene',
 'Ethylene Dichloride (1,2-Dichloroethane)',
 'Ethylene Oxide',
 'Ethylidene Dichloride (1,1-Dichloroethane)',
 'Formaldehyde',
 'Glycol Ethers',
 'Hexane',
 'Hexavalent Chromium',
 'Hydrochloric Acid',
 'Hydrogen Fluoride (Hydrofluoric Acid)',
 'Hydrogen Sulfide',
 'Isophorone',
 'Lead',
 'm-Xylene',
 'Maleic Anhydride',
 'Manganese',
 'Mercury',
 'Methane',
 'Methanol',
 'Methyl Chloride (Chloromethane)',
 'Methyl Chloroform (1,1,1-Trichloroethane)',
 'Methyl Isobutyl Ketone (4-Methyl-2-Pentanone)',
 'Methyl Methacrylate',
 'Methylene Chloride (Dichloromethane)',
 'Naphthalene',
 'Nickel',
 'Nitrogen Oxides',
 'Nitrous Oxide (N2O)',
 'o-Xylene',
 'Particulate Matter < 10 Microns, Filterable',
 'Particulate Matter < 2.5 Microns, Filterable',
 'Particulate Matter, Condensable',
 'Phenol',
 'Phosphorus',
 'Phthalic Anhydride',
 'Polycyclic Organic Matter',
 'Propionaldehyde',
 'Propylene Oxide',
 'Selenium',
 'Styrene',
 'Sulfur Oxides',
 'Sulfuric Acid',
 'Tetrachloroethylene (Perchloroethylene)',
 'Toluene',
 'Trichloroethylene',
 'Vinylidene Chloride (1,1-Dichloroethylene)',
 'Volatile Organic Compounds',
 'Xylenes (Isomers And Mixture)',
 'Grand Total']
data = data[cols]


#Loading in Pre1950s Housing values for additional features
pre1950housing = pd.read_csv('our-data/pre1950housing.xls-pre1950housing.csv')
h2016 = pre1950housing.copy()
h2016['Year'] = [2016]*len(h2016)
h2017 = pre1950housing.copy()
h2017['Year'] = [2017]*len(h2017)
h2018 = pre1950housing.copy()
h2018['Year'] = [2018]*len(h2018)
h2019 = pre1950housing.copy()
h2019['Year'] = [2019]*len(h2019)
pre1950 = pd.concat([h2016, h2017, h2018, h2019])
pre1950
data = data.merge(pre1950, how = 'left', left_on=['Geo_FIPS','YearOfContactDate'], right_on=['TRACT','Year'])





# Since we are dividing by 'Age0to17PopEst2015_19_x' to get percent variables we cannot have zeros for this values
data[data['Age0to17PopEst2015_19ACS_x']==0]


# When checking in the original utlization dataset, we see that values are 0 which means that this was not 
# an error from merging/cleaning  
merged_data[merged_data['TractFIPS_er']==40500]


#Thus, we drop data for this census tract
data = data[data['Age0to17PopEst2015_19ACS_x']!= 0]

#Separating out columns that we need to calculate a per100 value for
per100Cols = data.columns.to_list()[4:15]
#Removing for columns for those under 1 year old
per100Cols = per100Cols[0:3]+per100Cols[4:7]+per100Cols[8:]
per100Cols


# For each column from the utilization data related to Age 0 to 17 population, we divide per population estimate
# for this group

for col in per100Cols:
    data[col+'Per100'] = data[col]/data['Age0to17PopEst2015_19ACS_x']*100

data.iloc[:, -9:].isna().sum()


data.iloc[:, -9:]


data.isna().sum()


data.to_csv('test.csv')
