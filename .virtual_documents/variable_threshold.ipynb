


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn


#reading the data
data = pd.read_csv("data_W_1950s.csv")
columns = [data.columns]
print(columns)
print("Data shape:", data.shape)


data.head()


drop_col = ['YearOfContactDate','1,1,2-Trichloroethane','1,3-Butadiene','1,4-Dioxane (1,4-Diethyleneoxide)','Acrolein','Acrylic Acid','Antimony','Arsenic','Carbon Tetrachloride','Chloroethene (vinyl chloride)','Dibutylphthalate','Ethyl Chloride (Chloroethane)','Hexavalent Chromium','Isophorone','o-Xylene','Propylene Oxide','Selenium','Vinylidene Chloride (1,1-Dichloroethylene)','TRACT']
data = data.drop(columns=drop_col)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 200)
for i in data.columns:
    print(i)


#missingness check
data.isnull().sum().sort_values(ascending=False)


#Variance of all columns
variances = data.iloc[:, 2:].var()
pd.set_option('display.max_rows', 200)
# counting frequency for various thresholds level
variance_bins = pd.cut(variances, bins=20)
variance_freq = variance_bins.value_counts().sort_index()
variance_freq
#for i in variances:
    #print(i)


#scaling features for variance thresholding, otherwise variance across features varies too much
from sklearn.preprocessing import StandardScaler
data2 = data.iloc[:, 2:-1]
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data2)
feature_names = data2.columns
data_scaled


#Variance testing
from sklearn.feature_selection import VarianceThreshold
threshold = 1
selector = VarianceThreshold(threshold)
reduced_data = selector.fit_transform(data_scaled)
high_variance_features = feature_names[selector.get_support()]
reduced_data_df = pd.DataFrame(reduced_data, columns= high_variance_features, index= data2.index)
print('shape:',reduced_data_df.shape, 'features selected:', reduced_data_df.columns)
